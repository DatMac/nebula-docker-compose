x-airflow-common:
  &airflow-common
  image: my-airflow-image
  user: "${AIRFLOW_UID:-50000}"
  group_add:
    - "990" 
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://airflow:airflow@postgres_db:5432/airflow"
    AIRFLOW__CORE__FERNET_KEY: "A0C4Hjk1ToTD3_BkfQFSijHqH4pKXuw5UUM4yDuE1i0="
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__DEFAULT_TIMEZONE: "Asia/Bangkok"
    AIRFLOW_CONN_SPARK_DEFAULT: "spark://spark-master:7077"
    AIRFLOW_CONN_DOCKER_DEFAULT: "docker://local/var/run/docker.sock"
    PYSPARK_DRIVER_PYTHON: "/usr/local/bin/python3"
    PYSPARK_PYTHON: "/usr/local/bin/python3"
  volumes:
    - ./airflow/dags:/opt/airflow/dags:rw
    - ./airflow/logs:/opt/airflow/logs:rw
    - ./airflow/plugins:/opt/airflow/plugins:rw
    - ./spark/apps:/opt/spark_apps:ro
    - /var/run/docker.sock:/var/run/docker.sock
  networks:
    - nebula-net 
  depends_on:
    postgres_db:
      condition: service_healthy 

services:
  cassandra:
    image: cassandra:3.11
    container_name: cassandra
    hostname: cassandra
    ports:
      - "9042:9042"
      - "7070:7070" # Expose the JMX exporter port
    volumes:
      - ../cassandra-data/cassandra:/var/lib/cassandra
      # Mount the JMX exporter files
      - ./cassandra-monitoring:/opt/cassandra/jmx_exporter
    environment:
      # Add JVM_OPTS to load the java agent
      - JVM_OPTS=-javaagent:/opt/cassandra/jmx_exporter/jmx_prometheus_javaagent.jar=7070:/opt/cassandra/jmx_exporter/cassandra-jmx-config.yml
    networks:
      - nebula-net
    healthcheck:
      test: ["CMD", "cqlsh", "-e", "describe keyspaces"]
      interval: 30s
      timeout: 10s
      retries: 5

  metad0:
    image: docker.io/vesoft/nebula-metad:v3.8.0
    environment:
      USER: root
      TZ: "${TZ}"
    command:
      - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559
      - --local_ip=metad0
      - --ws_ip=metad0
      - --port=9559
      - --ws_http_port=19559
      - --data_path=/data/meta
      - --log_dir=/logs
      - --v=0
      - --minloglevel=0
      - --timezone_name=UTC+07:00
      - --default_parts_num=10
      - --default_replica_factor=1
      - --heartbeat_interval_secs=10
      - --agent_heartbeat_interval_secs=60
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://metad0:19559/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    ports:
      - 9559
      - 19559
      - 19560
    volumes:
      - /spark-tmp/nebula_hdfs/data/meta0:/data/meta
      - /spark-tmp/nebula_hdfs/logs/meta0:/logs
    networks:
      - nebula-net
    restart: on-failure
    cap_add:
      - SYS_PTRACE

  metad1:
    image: docker.io/vesoft/nebula-metad:v3.8.0
    environment:
      USER: root
      TZ: "${TZ}"
    command:
      - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559
      - --local_ip=metad1
      - --ws_ip=metad1
      - --port=9559
      - --ws_http_port=19559
      - --data_path=/data/meta
      - --log_dir=/logs
      - --v=0
      - --minloglevel=0
      - --timezone_name=UTC+07:00
      - --default_parts_num=10
      - --default_replica_factor=1
      - --heartbeat_interval_secs=10
      - --agent_heartbeat_interval_secs=60
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://metad1:19559/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    ports:
      - 9559
      - 19559
      - 19560
    volumes:
      - /spark-tmp/nebula_hdfs/data/meta1:/data/meta
      - /spark-tmp/nebula_hdfs/logs/meta1:/logs
    networks:
      - nebula-net
    restart: on-failure
    cap_add:
      - SYS_PTRACE

  metad2:
    image: docker.io/vesoft/nebula-metad:v3.8.0
    environment:
      USER: root
      TZ: "${TZ}"
    command:
      - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559
      - --local_ip=metad2
      - --ws_ip=metad2
      - --port=9559
      - --ws_http_port=19559
      - --data_path=/data/meta
      - --log_dir=/logs
      - --v=0
      - --minloglevel=0
      - --timezone_name=UTC+07:00
      - --default_parts_num=10
      - --default_replica_factor=1
      - --heartbeat_interval_secs=10
      - --agent_heartbeat_interval_secs=60
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://metad2:19559/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    ports:
      - 9559
      - 19559
      - 19560
    volumes:
      - /spark-tmp/nebula_hdfs/data/meta2:/data/meta
      - /spark-tmp/nebula_hdfs/logs/meta2:/logs
    networks:
      - nebula-net
    restart: on-failure
    cap_add:
      - SYS_PTRACE

  storaged0:
    image: docker.io/vesoft/nebula-storaged:v3.8.0
    environment:
      USER: root
      TZ: "${TZ}"
    command:
      - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559
      - --local_ip=storaged0
      - --ws_ip=storaged0
      - --port=9779
      - --ws_http_port=19779
      - --data_path=/data/storage
      - --log_dir=/logs
      - --v=0
      - --minloglevel=0
      - --timezone_name=UTC+07:00
      - --heartbeat_interval_secs=10
      - --raft_heartbeat_interval_secs=30
      - --raft_rpc_timeout_ms=500
      - --wal_ttl=14400
      - --disable_page_cache=false
      - --enable_rocksdb_prefix_filtering=true
      - --enable_rocksdb_whole_key_filtering=true
      - --enable_partitioned_index_filter=false
      - --query_concurrently=true
      - --num_io_threads=16
      - --num_max_connections=0
      - --num_worker_threads=32
      - --max_concurrent_subtasks=10
      - --memory_tracker_limit_ratio=0.8
    depends_on:
      - metad0
      - metad1
      - metad2
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://storaged0:19779/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    ports:
      - 9779
      - 19779
      - 19780
    volumes:
      - /spark-tmp/nebula_hdfs/data/storage0:/data/storage
      - /spark-tmp/nebula_hdfs/logs/storage0:/logs
    networks:
      - nebula-net
    restart: on-failure
    cap_add:
      - SYS_PTRACE

  storaged1:
    image: docker.io/vesoft/nebula-storaged:v3.8.0
    environment:
      USER: root
      TZ: "${TZ}"
    command:
      - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559
      - --local_ip=storaged1
      - --ws_ip=storaged1
      - --port=9779
      - --ws_http_port=19779
      - --data_path=/data/storage
      - --log_dir=/logs
      - --v=0
      - --minloglevel=0
      - --timezone_name=UTC+07:00
      - --heartbeat_interval_secs=10
      - --raft_heartbeat_interval_secs=30
      - --raft_rpc_timeout_ms=500
      - --wal_ttl=14400
      - --disable_page_cache=false
      - --enable_rocksdb_prefix_filtering=true
      - --enable_rocksdb_whole_key_filtering=true
      - --enable_partitioned_index_filter=false
      - --query_concurrently=true
      - --num_io_threads=16
      - --num_max_connections=0
      - --num_worker_threads=32
      - --max_concurrent_subtasks=10
      - --memory_tracker_limit_ratio=0.8
    depends_on:
      - metad0
      - metad1
      - metad2
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://storaged1:19779/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    ports:
      - 9779
      - 19779
      - 19780
    volumes:
      - /spark-tmp/nebula_hdfs/data/storage1:/data/storage
      - /spark-tmp/nebula_hdfs/logs/storage1:/logs
    networks:
      - nebula-net
    restart: on-failure
    cap_add:
      - SYS_PTRACE

  storaged2:
    image: docker.io/vesoft/nebula-storaged:v3.8.0
    environment:
      USER: root
      TZ: "${TZ}"
    command:
      - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559
      - --local_ip=storaged2
      - --ws_ip=storaged2
      - --port=9779
      - --ws_http_port=19779
      - --data_path=/data/storage
      - --log_dir=/logs
      - --v=0
      - --minloglevel=0
      - --timezone_name=UTC+07:00
      - --heartbeat_interval_secs=10
      - --raft_heartbeat_interval_secs=30
      - --raft_rpc_timeout_ms=500
      - --wal_ttl=14400
      - --disable_page_cache=false
      - --enable_rocksdb_prefix_filtering=true
      - --enable_rocksdb_whole_key_filtering=true
      - --enable_partitioned_index_filter=false
      - --query_concurrently=true
      - --num_io_threads=16
      - --num_max_connections=0
      - --num_worker_threads=32
      - --max_concurrent_subtasks=10
      - --memory_tracker_limit_ratio=0.8
    depends_on:
      - metad0
      - metad1
      - metad2
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://storaged2:19779/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    ports:
      - 9779
      - 19779
      - 19780
    volumes:
      - /spark-tmp/nebula_hdfs/data/storage2:/data/storage
      - /spark-tmp/nebula_hdfs/logs/storage2:/logs
    networks:
      - nebula-net
    restart: on-failure
    cap_add:
      - SYS_PTRACE


  graphd:
    image: docker.io/vesoft/nebula-graphd:v3.8.0
    environment:
      USER: root
      TZ: "${TZ}"
    command:
      - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559
      - --port=9669
      - --local_ip=graphd
      - --ws_ip=graphd
      - --ws_http_port=19669
      - --log_dir=/logs
      - --v=0
      - --minloglevel=0
      - --timezone_name=UTC+07:00
      - --default_collate=utf8_bin
      - --accept_partial_success=false
      - --session_reclaim_interval_secs=60
      - --num_accept_threads=1
      - --num_netio_threads=0
      - --num_max_connections=0
      - --num_worker_threads=0
      - --heartbeat_interval_secs=10
      - --slow_query_threshold_us=200000
      - --enable_authorize=true
      - --auth_type=password
      - --system_memory_high_watermark_ratio=0.8
      - --enable_space_level_metrics=true
      - --enable_experimental_feature=true
      - --enable_data_balance=true
      - --memory_tracker_limit_ratio=0.8
      - --max_job_size=1
      - --min_batch_size=8192
      - --optimize_appendvertices=false
      - --path_batch_size=10000
      - --session_idle_timeout_secs=300
      - --client_idle_timeout_secs=300
    depends_on:
      - storaged0
      - storaged1
      - storaged2
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://graphd:19669/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    ports:
      - "9669:9669"
      - 19669
      - 19670
    volumes:
      - /spark-tmp/nebula_hdfs/logs/graph:/logs
    networks:
      - nebula-net
    restart: on-failure
    cap_add:
      - SYS_PTRACE

  graphd1:
    image: docker.io/vesoft/nebula-graphd:v3.8.0
    environment:
      USER: root
      TZ: "${TZ}"
    command:
      - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559
      - --port=9669
      - --local_ip=graphd1
      - --ws_ip=graphd1
      - --ws_http_port=19669
      - --log_dir=/logs
      - --v=0
      - --minloglevel=0
      - --timezone_name=UTC+07:00
      - --default_collate=utf8_bin
      - --accept_partial_success=false
      - --session_reclaim_interval_secs=60
      - --num_accept_threads=1
      - --num_netio_threads=0
      - --num_max_connections=0
      - --num_worker_threads=0
      - --heartbeat_interval_secs=10
      - --slow_query_threshold_us=200000
      - --enable_authorize=true
      - --auth_type=password
      - --system_memory_high_watermark_ratio=0.8
      - --enable_space_level_metrics=true
      - --enable_experimental_feature=true
      - --enable_data_balance=true
      - --memory_tracker_limit_ratio=0.8
      - --max_job_size=1
      - --min_batch_size=8192
      - --optimize_appendvertices=false
      - --path_batch_size=10000
      - --session_idle_timeout_secs=300
      - --client_idle_timeout_secs=300
    depends_on:
      - storaged0
      - storaged1
      - storaged2
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://graphd1:19669/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    ports:
      - 9669
      - 19669
      - 19670
    volumes:
      - /spark-tmp/nebula_hdfs/logs/graph1:/logs
    networks:
      - nebula-net
    restart: on-failure
    cap_add:
      - SYS_PTRACE

  graphd2:
    image: docker.io/vesoft/nebula-graphd:v3.8.0
    environment:
      USER: root
      TZ: "${TZ}"
    command:
      - --meta_server_addrs=metad0:9559,metad1:9559,metad2:9559
      - --port=9669
      - --local_ip=graphd2
      - --ws_ip=graphd2
      - --ws_http_port=19669
      - --log_dir=/logs
      - --v=0
      - --minloglevel=0
      - --timezone_name=UTC+07:00
      - --default_collate=utf8_bin
      - --accept_partial_success=false
      - --session_reclaim_interval_secs=60
      - --num_accept_threads=1
      - --num_netio_threads=0
      - --num_max_connections=0
      - --num_worker_threads=0
      - --heartbeat_interval_secs=10
      - --slow_query_threshold_us=200000
      - --enable_authorize=true
      - --auth_type=password
      - --system_memory_high_watermark_ratio=0.8
      - --enable_space_level_metrics=true
      - --enable_experimental_feature=true
      - --enable_data_balance=true
      - --memory_tracker_limit_ratio=0.8
      - --max_job_size=1
      - --min_batch_size=8192
      - --optimize_appendvertices=false
      - --path_batch_size=10000
      - --session_idle_timeout_secs=300
      - --client_idle_timeout_secs=300
    depends_on:
      - storaged0
      - storaged1
      - storaged2
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://graphd2:19669/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    ports:
      - 9669
      - 19669
      - 19670
    volumes:
      - /spark-tmp/nebula_hdfs/logs/graph2:/logs
    networks:
      - nebula-net
    restart: on-failure
    cap_add:
      - SYS_PTRACE

  console:
    image: docker.io/vesoft/nebula-console:v3.6.0
    entrypoint: ""
    command:
      - sh
      - -c
      - |
        for i in `seq 1 60`;do
          var=`nebula-console -addr graphd -port 9669 -u root -p nebula -e 'ADD HOSTS "storaged0":9779,"storaged1":9779,"storaged2":9779'`;
          if [[ $$? == 0 ]];then
            break;
          fi;
          sleep 1;
          echo "retry to add hosts.";
        done && tail -f /dev/null;
    depends_on:
      - graphd
    networks:
      - nebula-net

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
    container_name: namenode
    hostname: namenode
    restart: always
    ports:
      - "9870:50070"
      - "9000:9000"
      - "8020:8020"
    volumes:
      - /spark-tmp/nebula_hdfs/data/hdfs/namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    networks:
      - nebula-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://namenode:50070"]
      interval: 30s
      timeout: 10s
      retries: 3

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
    container_name: datanode
    hostname: datanode
    restart: always
    volumes:
      - /spark-tmp/nebula_hdfs/data/hdfs/datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:50070"
      CORE_CONF_fs_defaultFS: "hdfs://namenode:8020"
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - nebula-net

  spark-master:
    image: my-spark-app-image
    user: "1001:1001"
    environment:
      - SPARK_MODE=master
      - SPARK_OPTS=--conf spark.driver.memory=2g --conf spark.driver.cores=1 --conf spark.hadoop.fs.defaultFS=hdfs://namenode:8020
      - PYSPARK_PYTHON=/usr/local/bin/python3
    mem_limit: 16g
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./spark/apps:/opt/bitnami/spark/apps
      - ./spark/data:/opt/bitnami/spark/data
      - ./spark/config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - /spark-tmp:/opt/bitnami/spark/tmp-dir
    depends_on:
      - namenode
      - datanode
    networks:
      - nebula-net

  spark-worker:
    image: my-spark-app-image
    user: "1001:1001"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_OPTS=--conf spark.hadoop.fs.defaultFS=hdfs://namenode:8020
      - SPARK_WORKER_MEMORY=4g
      - SPARK_WORKER_CORES=2
      - PYSPARK_PYTHON=/usr/local/bin/python3
    depends_on:
      - spark-master
    volumes:
      - ./spark/apps:/opt/bitnami/spark/apps
      - ./spark/data:/opt/bitnami/spark/data
      - ./spark/config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - /spark-tmp:/opt/bitnami/spark/tmp-dir
    networks:
      - nebula-net

  postgres_db:
    image: postgres:13
    container_name: airflow_postgres_db
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - airflow_postgres_meta_data:/var/lib/postgresql/data
    networks:
      - nebula-net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  airflow-init:
    <<: *airflow-common
    container_name: airflow_init
    command:
      - bash
      - -c
      - |
        # This script is idempotent, meaning it can be run multiple times safely.
        
        # 1. Initialize or upgrade the Airflow database
        echo "Initializing or upgrading the Airflow database..."
        airflow db upgrade
        
        # 2. Check if the 'admin' user already exists
        echo "Checking for admin user..."
        if ! airflow users list | grep -q "admin"; then
          # Create the admin user only if it does not exist
          echo "Admin user not found. Creating admin user..."
          airflow users create \
            --username admin \
            --firstname Admin \
            --lastname User \
            --role Admin \
            --email admin@example.com \
            --password admin
        else
          echo "Admin user already exists. Skipping creation."
        fi
        
        # 3. Add or update the Spark connection (this command is idempotent by default)
        # echo "Adding/updating connection..."
        # airflow connections add 'spark-spark' --conn-type 'spark' --conn-host 'spark://spark-master' --conn-port '7077' --conn-extra '{"master": "spark://spark-master:7077"}';
    user: "0:0"
    depends_on:
      postgres_db:
        condition: service_healthy

  airflow-webserver:
    <<: *airflow-common
    container_name: airflow_webserver
    command: webserver
    ports:
      - "8081:8080" 
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 90s 
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow_scheduler
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $$(hostname)"]
      interval: 30s
      timeout: 10s
      retries: 5
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  triton-server:
    image: nvcr.io/nvidia/tritonserver:24.01-py3
    container_name: triton-server
    hostname: triton-server
    restart: always
    ports:
      - "8000:8000" # HTTP
      - "8001:8001" # gRPC
      - "8002:8002" # Metrics
    volumes:
      - triton-model-repo:/models
    networks:
      - nebula-net
    command: tritonserver --model-repository=/models --model-control-mode=poll --repository-poll-secs=30 --log-verbose=1
    depends_on:
      namenode:
        condition: service_healthy
      datanode:
        condition: service_healthy

  model-repository-syncer:
    image: pyg-node
    container_name: model-repository-syncer
    hostname: model-repository-syncer
    restart: always
    volumes:
      - ./src:/app # Assuming sync script is in src
      - triton-model-repo:/models
      - ./hdfs-config/core-site.xml:/opt/hadoop-2.7.4/etc/hadoop/core-site.xml
      - ./hdfs-config/hdfs-site.xml:/opt/hadoop-2.7.4/etc/hadoop/hdfs-site.xml
    environment:
      - JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
      - HADOOP_CONF_DIR=/opt/hadoop-2.7.4/etc/hadoop
      - HDFS_MODEL_REPO=/triton_models # The source directory on HDFS
      - LOCAL_MODEL_REPO=/models        # The target directory in the shared volume
    networks:
      - nebula-net
    entrypoint: /bin/bash
    command: /app/sync_models.sh # Path to the sync script
    depends_on:
      namenode:
        condition: service_healthy
      datanode:
        condition: service_healthy

  prometheus:
    image: prom/prometheus:v2.51.2
    container_name: prometheus
    hostname: prometheus
    restart: always
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    networks:
      - nebula-net
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    depends_on:
      - triton-server

  grafana:
    image: grafana/grafana:10.4.2
    container_name: grafana
    hostname: grafana
    restart: always
    ports:
      - "3000:3000"
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning
      - grafana-data:/var/lib/grafana
    networks:
      - nebula-net
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin # Change in a real production environment
    depends_on:
      - prometheus

  nebula-stats-exporter:
    image: vesoft/nebula-stats-exporter:v3.3.0
    container_name: nebula-stats-exporter
    hostname: nebula-stats-exporter
    restart: always
    ports:
      - "9100:9100"
    volumes:
      - ./nebula-export/config.yaml:/config.yaml
    networks:
      - nebula-net
    command:
      - '--bare-metal'
      - '--bare-metal-config=/config.yaml'
    depends_on:
      - metad0
      - metad1
      - metad2
      - storaged0
      - storaged1
      - storaged2
      - graphd
      - graphd1
      - graphd2

  inference-api:
    image: inference-api
    container_name: inference-api
    hostname: inference-api
    restart: always
    ports:
      - "8501:8000"
    networks:
      - nebula-net
    environment:
      - NEBULA_HOST=graphd
      - CASSANDRA_HOSTS=cassandra
      - TRITON_URL=http://triton-server:8000/v2/models/graphsage_model/infer
      - NEBULA_VERTEX_TAG=Customer         
      - NEBULA_LOOKUP_PROPERTY=cust_id     
    depends_on:
      triton-server:
        condition: service_started
      graphd:
        condition: service_healthy
      cassandra:
        condition: service_healthy

networks:
  nebula-net:
    driver: bridge

volumes:
  airflow_postgres_meta_data:
    driver: local
  triton-model-repo:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local

from __future__ import annotations

import pendulum
from airflow.models.dag import DAG
from airflow.operators.bash import BashOperator
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

with DAG(
    dag_id="spark_word_count_test",
    schedule=None,  # We will trigger this manually for testing
    start_date=pendulum.datetime(2023, 1, 1, tz="UTC"),
    catchup=False,
    doc_md="""
    ## Spark Word Count Test DAG
    
    1. Submits a PySpark word count job.
    2. The Spark job writes its output to HDFS.
    3. A second task reads the output from HDFS to verify success.
    """,
    tags=["spark", "test"],
) as dag:
    
    # Define the output path using an Airflow Jinja template.
    # This ensures each run has a unique output folder.
    hdfs_output_path = "hdfs://namenode:8020/tmp/word_count_{{ ts_nodash }}"

    # Task to submit the Spark job
    submit_spark_job = SparkSubmitOperator(
        task_id="submit_spark_word_count",
        conn_id="spark-spark",  # Uses the connection we set up in docker-compose
        
        # Path to the Python script *inside the Airflow/Spark containers*
        application="/opt/spark_apps/word_count.py",
        
        # Pass the unique HDFS output path as an argument to the script
        application_args=[hdfs_output_path],
        
        # Resources for the Spark job (optional, but good practice)
        num_executors=1,
        executor_cores=1,
        executor_memory="512m",
        
        # Use the correct verbosity for Spark 2.4 logs
        verbose=True,
    )

    # Task to verify the output by reading it from HDFS
    verify_spark_output = BashOperator(
        task_id="verify_spark_output_in_hdfs",
        bash_command=f"hdfs dfs -cat {hdfs_output_path}/*",
        doc_md="""
        Reads the output files generated by the Spark job from HDFS.
        The `*` is needed because Spark writes output in part-files (e.g., part-00000).
        This command will fail if the output doesn't exist, causing the task to fail.
        """,
    )

    # Set the task dependency: verification runs only after the Spark job succeeds
    submit_spark_job >> verify_spark_output

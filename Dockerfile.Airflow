FROM apache/airflow:2.6.0-python3.7

# --- Environment Variables ---
# Set up all environment variables first for clarity and use in later commands.
ENV JAVA_HOME="/opt/java/openjdk-8"
ENV HADOOP_HOME="/opt/hadoop-2.7.4"
ENV HADOOP_CONF_DIR="${HADOOP_HOME}/etc/hadoop"
ENV SPARK_HOME="/opt/spark-2.4.4"
# Add all necessary bin directories to the PATH
ENV PATH="${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${SPARK_HOME}/bin:${PATH}"

# --- Arguments for Download URLs ---
# We still need the JDK URL, but Hadoop and Spark will be copied from local files.
ARG JDK_VERSION="8u422-b05"
ARG JDK_URL="https://github.com/adoptium/temurin8-binaries/releases/download/jdk${JDK_VERSION}/OpenJDK8U-jdk_x64_linux_hotspot_8u422b05.tar.gz"

# Switch to root to perform all installations
USER root

# --- Copy Local Archives ---
# Copy your pre-downloaded Hadoop and Spark archives into a temporary location in the image.
# Make sure these files are in the same directory as your Dockerfile when you run the build.
COPY hadoop-2.7.4.tar.gz /tmp/hadoop.tar.gz
COPY spark-2.4.4-bin-hadoop2.7.tgz /tmp/spark.tgz

# --- Installation Block ---
RUN set -eux; \
    # 1. Install dependencies
    apt-get update; \
    apt-get install -y --no-install-recommends curl ca-certificates; \
    \
    # 2. Install Java 8
    mkdir -p "${JAVA_HOME}"; \
    curl --fail --location --output /tmp/openjdk.tar.gz "${JDK_URL}"; \
    tar -xzf /tmp/openjdk.tar.gz -C "${JAVA_HOME}" --strip-components=1; \
    rm /tmp/openjdk.tar.gz; \
    \
    # 3. Install Hadoop 2.7.4 Client from the copied archive
    mkdir -p "${HADOOP_HOME}"; \
    tar -xzf /tmp/hadoop.tar.gz -C "${HADOOP_HOME}" --strip-components=1; \
    rm /tmp/hadoop.tar.gz; \
    \
    # 4. Install Spark 2.4.4 Client from the copied archive
    mkdir -p "${SPARK_HOME}"; \
    tar -xzf /tmp/spark.tgz -C "${SPARK_HOME}" --strip-components=1; \
    rm /tmp/spark.tgz; \
    \
    # 5. Configure Hadoop to find Java
    echo "export JAVA_HOME=${JAVA_HOME}" >> "${HADOOP_CONF_DIR}/hadoop-env.sh"; \
    \
    # 6. Clean up apt cache
    rm -rf /var/lib/apt/lists/*

# --- Hadoop Cluster Configuration ---
# This part remains the same, assuming you have a 'hadoop_conf' directory to copy.
COPY ./hadoop_conf/ ${HADOOP_CONF_DIR}/

# Switch back to the non-privileged airflow user
USER airflow

# --- Python Dependencies ---
RUN pip install --no-cache-dir apache-airflow-providers-apache-spark==2.1.3 apache-airflow-providers-docker==3.7.0

services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
    container_name: namenode
    hostname: namenode
    restart: always
    ports:
      - "9870:50070"
      - "9000:9000"
      - "8020:8020"
    volumes:
      - /nebula_hdfs/data/hdfs/namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    networks:
      - pyg-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://namenode:50070"]
      interval: 30s
      timeout: 10s
      retries: 3

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
    container_name: datanode
    hostname: datanode
    restart: always
    volumes:
      - /nebula_hdfs/data/hdfs/datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:50070"
      CORE_CONF_fs_defaultFS: "hdfs://namenode:8020"
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - pyg-net

  job-init:
    image: pyg-node
    entrypoint: /bin/bash
    volumes:
      - ./hdfs-config/core-site.xml:/opt/hadoop-2.7.4/etc/hadoop/core-site.xml
      - ./hdfs-config/hdfs-site.xml:/opt/hadoop-2.7.4/etc/hadoop/hdfs-site.xml
    environment:
      - JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
      - HADOOP_CONF_DIR=/opt/hadoop-2.7.4/etc/hadoop
      - CLASSPATH=$(/opt/hadoop-2.7.4/bin/hadoop classpath --glob)
    depends_on:
      namenode:
        condition: service_healthy
      datanode:
        condition: service_healthy
    networks:
      - pyg-net
    command: >
      -c "echo '--- job-init: Setting up HDFS client environment ---' &&
          export CLASSPATH=$$(/opt/hadoop-2.7.4/bin/hadoop classpath --glob) &&
          /opt/hadoop-2.7.4/bin/hdfs dfs -ls / &&
          echo '--- job-init: Executing cleanup command ---' &&
          /opt/hadoop-2.7.4/bin/hdfs dfs -rm -r -f /job-coordination/* &&
          echo '--- job-init: Cleanup finished successfully ---'"  
  
  pyg-master:
    image: pyg-node
    container_name: pyg-master
    hostname: pyg-master
    shm_size: '16g'
    volumes:
      - ./src:/app
      - ./hdfs-config/core-site.xml:/opt/hadoop-2.7.4/etc/hadoop/core-site.xml
      - ./hdfs-config/hdfs-site.xml:/opt/hadoop-2.7.4/etc/hadoop/hdfs-site.xml
    environment:
      - NUM_WORKERS=3
      - HDFS_COORD_PATH=/job-coordination
      - HDFS_DATA_PATH=/tmp/pyg_dataset # HDFS source path
      - LOCAL_DATA_PATH=/pyg_dataset    # Local destination path in container
      - MASTER_ADDR=pyg-master
      - MASTER_PORT=29500
    depends_on:
      job-init:
        condition: service_completed_successfully
    networks:
      - pyg-net
    command: >
        --num_epochs=10
        --batch_size=512
        --num_workers=0
        --progress_bar
        --model_save_path /app/saved_models

  pyg-worker:
    image: pyg-node
    shm_size: '16g'
    volumes:
      - ./src:/app
      - ./hdfs-config/core-site.xml:/opt/hadoop-2.7.4/etc/hadoop/core-site.xml
      - ./hdfs-config/hdfs-site.xml:/opt/hadoop-2.7.4/etc/hadoop/hdfs-site.xml
    environment:
      - NUM_WORKERS=3
      - HDFS_COORD_PATH=/job-coordination
      - HDFS_DATA_PATH=/tmp/pyg_dataset
      - LOCAL_DATA_PATH=/pyg_dataset
      - MASTER_ADDR=pyg-master
      - MASTER_PORT=29500
    depends_on:
      pyg-master: # Wait for the master to be running
        condition: service_started
    networks:
      - pyg-net
    command: >
        --num_epochs=10
        --batch_size=512
        --num_workers=0
        --progress_bar
        --model_save_path /app/saved_models

volumes:
  hdfs-data:

networks:
  pyg-net:
    driver: bridge
